# Bike Shop Database Design (PostgreSQL)

This document summarises the PostgreSQL schema defined in `schema.sql`. The aim
is to normalise the CSV exports from `AllcsvData.xlsx` and to preserve the
relationships described in `RelationsInDataCSV.txt`.

## Entity Relationships

```
brands      1 ----< products >---- 1 categories
                            |
customers   1 ----< orders  >---- 1 stores
                            |
                            v
                           staffs (self-referencing manager_id)

orders      1 ----< order_items >---- 1 products
stores      1 ----< stocks      >---- 1 products
```

Key foreign keys:
- `orders.customer_id` -> `customers.customer_id`
- `orders.store_id` -> `stores.store_id`
- `orders.staff_id` -> `staffs.staff_id`
- `order_items.product_id` -> `products.product_id`
- `products.brand_id` -> `brands.brand_id`
- `products.category_id` -> `categories.category_id`
- `stocks.store_id` -> `stores.store_id`
- `staffs.manager_id` -> `staffs.staff_id` (nullable self reference)

## Table Highlights

- `stores.store_id` and `staffs.staff_id` use PostgreSQL `GENERATED BY DEFAULT AS IDENTITY`.
  The ETL loader resets these sequences to `MAX(id) + 1` after each load.
- Dates in `orders` are stored as `DATE`. The transform stage parses `DD/MM/YYYY`
  strings into proper timestamps before loading.
- `order_items.discount` is a numeric fraction (for example `0.10` for 10%).
- `schema.sql` creates an analytic view `vw_order_details` that joins orders,
  customers, stores, staff and order items.
- The ETL pipeline also maintains an `order_summary` table (not part of
  `schema.sql`) that stores per-order totals for quick reporting.

## ETL Flow

1. **Extract (`Extract.py`)**  
   Reads the CSV files from the workspace, normalises column names and ensures
   consistent typing.

2. **Transform (`Transform.py`)**  
   - Creates surrogate IDs for stores and staff.
   - Maps store names and staff names in other tables to those IDs.
   - Converts string dates to `datetime64`.
   - Ensures every column matches the PostgreSQL types expected by the schema.

3. **Load (`Load.py`)**  
   - `_ensure_schema` runs on every load to create tables and indexes with
     `CREATE TABLE/INDEX IF NOT EXISTS`, so a fresh database works immediately.
   - Tables are truncated in dependency order with `RESTART IDENTITY CASCADE`.
   - Bulk inserts use `psycopg2.extras.execute_values`, with NumPy and pandas
     scalars coerced to plain Python types.
   - Identity sequences are synchronised to existing maxima.
   - `order_summary` is dropped, recreated and repopulated with the latest
     aggregates.

## Operational Checklist

- Initialise a database by running `psql -f schema.sql` or simply rely on the
  loader to build the schema automatically.
- Provide PostgreSQL connection settings through the `POSTGRES_*` environment
  variables (defaults are in `Load.py`).
- After running `python main.py`, inspect table counts and spot-check the
  `order_summary` totals against the source CSV data.

## Sample Queries

```sql
-- Revenue per store
SELECT s.store_name,
       SUM(oi.quantity * oi.list_price * (1 - oi.discount)) AS revenue
FROM orders o
JOIN stores s ON s.store_id = o.store_id
JOIN order_items oi ON oi.order_id = o.order_id
GROUP BY s.store_id, s.store_name
ORDER BY revenue DESC;

-- Inventory snapshot per store
SELECT s.store_name,
       p.product_name,
       st.quantity
FROM stocks st
JOIN stores s   ON s.store_id   = st.store_id
JOIN products p ON p.product_id = st.product_id
ORDER BY s.store_name, p.product_name;

-- Order summary produced by the pipeline
SELECT *
FROM order_summary
ORDER BY order_date DESC, order_id DESC;
```
